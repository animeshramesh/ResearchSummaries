**1. Introduction**
* Choice of data representation important.
* We need to make learning algorithms less dependent on feature engineering.
* Representation learning : learning representations of data that make it easier to extract useful information when building classifiers/prediction models.

**2. Why care about Representations?**
* Speech recognition and signal processing, Microsoft MAVIS 2012
* Object Recognition
* NLP : Distributed representation : word embedding (words mapped to vectors), SENNA system, Google image search : Images and queries are in the same space.
* Multi-Task, Transfer Learning, Domain adaptation 
	
**3. What makes a good representation?**
* **3.1 Priors for Representation Learning in AI**
	* Smoothness : If x~y, then f(x)~f(y)
	* Multiple explanatory factors : data is generated by several factors. What you learn about one factor can be generalized in many configurations of other factors. So, objective is to extract these factors of variation.
	* Hierarchical organization of explanatory factors : abstract concepts - higher in the hierarchy.
	* Semi-supervised learning : Representations which explain P(X) can also be used to explain P(Y|X) -> sharing statistical strength between unsupervised and supervised learning.
	* Shared factors across tasks.
	* Manifolds : Probability mass conecntrates near regions that have a much smaller dimensionality than the inmput data.
	* Natural clustering : *Manifold Tangent Classifier?*		- Temporal and spatial coherence : Spatially/temporally nearby observations tend to be similar. So penalizing changes in values of time and space is one way to go.
	* Sparsity : Only a small fraction of possible factors are relevant. 
	* Simplicity of Factor dependencies : In high level representations, factors are related to one another using simple, linear dependencies.	
* **3.2 Smoothness and the curse of dimensionality**
	* Assumption is that the traget function is smooth enoguh so that we can rely on examples to explicitly map out the wrinkles of the target function.
	* But in high-dimensional features, there are too many "wrinkles" - ups and downs. Generalization achieved by smooth interpolation between neighbouring training examples.
	* Learning algorithms must be flexible and non-paramatric. 
* **3.3 Distributed representations**
	* Expressive representations - can capture a huge number of possible input configs.
	* One-hot representations need O(N) parameters to distinguish O(N) input regions.
	* RBMS, autoencoders, multi-layer networks can distinguish O(N) input regions using O(2^k) params. (k = nonzero elements in sparse representation) -> motivation between distributed/sparse representations.
	* In distributed representations, an exponential subset of features can be activated.
* **3.4 Depth and abstraction**
	* Deep architectures - difficult to train.
	* Deep architectures : promote reuse of features, leads to abstract features at higher layer of representations.
	* More abstract concepts can be constructed from less abstract ones and they are invariant to local changes in the input -> highly non-linear functions of the raw input.		
* **3.5 Disentangling factors of variation**		
	* Disentangle as many factors as possible, discarding as little information about the data.
	
	
**9. Evaluating and Monitoring Performance**
* Easy to evaluate a feature learning algo for a specific task. But we need to generalize this approach.
* Hence, use a completely unsupervised approach to evaluate performance.
* For auto-encoders :
	* Test reconstruction error can be used to measure performance. But this is misleading since we can just reduce this error by increasing the size of the features.
	* Denoising reconstruction error solves the above problem. 
* For RBMs:
	* *Annealed Importance Sampling* ???(Murray and Salkhutdinov, 2009) - to estimate the partition function. But is unreliable since its has high variance.
	* Track the partition function during training.
	* For toy-RBMs, exact log-likelihood can be computed analytically.

**10. Global Training of Deep Models**
* 10.1 The Challenge of Training Deep Architectures
	* Higher-level abstraction = more non-linearity.
		
**11. Building-in invariance.**
* Incorporating prior knowledge is beneficial.
* How basic domain knowledge of input may be used to learn better features.
* 11.1 Generating transformed examples : Augment training data by including samples with slightly more deformations to original samples. (Translation, scale, shear, rotation invariance)
* 11.2 Convolutions and pooling
	* Exploit the topological structure (2D in images)
	* Similar input values at nearby values. (local receptive field)
	* Output of convolution -> feature map. 
	* Output of pooling is invariant to small variations in the input.
	* Patch based training. 
	* Convolutional and tiled convolutional training:
		* Denoising images -> Denoising AE
		* Probabilistic max-pooling operation
		* Convolutional version of sparse coding -> deconvolutional networks
		* Tiled convolution -> parameters are shared only between feature extractors that are 'k' steps away. This allows pooling units to be invariant to more than just translations.
	* Alternatives to pooling :
		* Scattering operators : has convolutions and pooling/sparse, high dimensional features/features are not learnt, they are set. 
* 11.3 Temporal coherance and slow features
	* Features which are temporally close together tend to be similar.
	* Temporal coherance prior can be represented as :
		* Squared diff between features at t and t+1
		* Absolute value in the above case -> enforce that difference should be 0.
		* *Something on time scale. Didn't understand.*
		* Features tend to assimilate in groups. (Position x,y,z at time t) and these groups tend to move together.
			
